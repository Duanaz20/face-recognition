{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12347836,"sourceType":"datasetVersion","datasetId":7784324}],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/duaanaz/face-recognition?scriptVersionId=251890935\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# *Face Recognition Project*","metadata":{}},{"cell_type":"code","source":"# Load important libraries\nimport os\nimport cv2\nimport numpy as np\nimport pandas as pd\nfrom glob import glob\nimport matplotlib.pyplot as plt\nfrom skimage.feature import hog\nfrom skimage import exposure\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n\n# Set the plotting style\nplt.style.use('ggplot')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T17:55:41.829388Z","iopub.execute_input":"2025-07-22T17:55:41.82967Z","iopub.status.idle":"2025-07-22T17:55:41.835945Z","shell.execute_reply.started":"2025-07-22T17:55:41.829651Z","shell.execute_reply":"2025-07-22T17:55:41.834954Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load pre-trained Haar Cascade classifiers for face, eyes, smile, and nose detection\n\nface_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\neye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_eye.xml\")\nsmile_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_smile.xml\")\n# nose_cascade = cv2.CascadeClassifier(os.path.expanduser(\"~/haarcascade_mcs_nose.xml\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T17:55:41.852741Z","iopub.execute_input":"2025-07-22T17:55:41.853141Z","iopub.status.idle":"2025-07-22T17:55:41.900506Z","shell.execute_reply.started":"2025-07-22T17:55:41.853113Z","shell.execute_reply":"2025-07-22T17:55:41.899654Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"base_path = '../input/students-images/'\nstudent_ids= sorted(os.listdir(base_path))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T17:55:41.902004Z","iopub.execute_input":"2025-07-22T17:55:41.902307Z","iopub.status.idle":"2025-07-22T17:55:41.907322Z","shell.execute_reply.started":"2025-07-22T17:55:41.902286Z","shell.execute_reply":"2025-07-22T17:55:41.906401Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Exploratory Data Analysis\n**Data Frame**","metadata":{}},{"cell_type":"code","source":"# Make data frame\ndf = []\nfor std_id in student_ids:\n    paths = sorted(glob(os.path.join(base_path,std_id,std_id,'*.jpg'))) \n    for p in paths:\n        img = cv2.imread(p)\n        if img is None:\n            print(\"there is no image\")\n            continue\n        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n        df.append({\n            'student_id': std_id,\n            'path': p,\n            'height': img.shape[0],\n            'width': img.shape[1],\n            'aspect_ratio': img.shape[1] / img.shape[0],\n            'brightness': np.mean(gray),\n            'contrast': np.std(gray),\n            'blur_score': cv2.Laplacian(gray, cv2.CV_64F).var()\n        })\n\n# Convert the list to a DataFrame\ndf = pd.DataFrame(df)\n\n# print data frame\nprint(df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T17:55:41.90831Z","iopub.execute_input":"2025-07-22T17:55:41.908556Z","iopub.status.idle":"2025-07-22T17:57:47.463361Z","shell.execute_reply.started":"2025-07-22T17:55:41.908537Z","shell.execute_reply":"2025-07-22T17:57:47.462584Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Assessing Data Quantity","metadata":{}},{"cell_type":"code","source":"# Print number of images per student\nprint(\"Number of images per student:\")\nprint(df['student_id'].value_counts().sort_index())\n\n# Visualizations of: \n# No. of Images per student \nplt.figure(figsize=(10, 4))\ndf['student_id'].value_counts().sort_index().plot(kind='bar', color='skyblue')\nplt.title(\"Number of Images per Student\")\nplt.xlabel(\"Student ID\")\nplt.ylabel(\"Number of Images\")\nplt.grid(axis='y')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T17:57:47.465031Z","iopub.execute_input":"2025-07-22T17:57:47.465288Z","iopub.status.idle":"2025-07-22T17:57:47.669304Z","shell.execute_reply.started":"2025-07-22T17:57:47.465267Z","shell.execute_reply":"2025-07-22T17:57:47.668432Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# *Face Detection and Cropping with Grayscale Conversion*","metadata":{}},{"cell_type":"code","source":"# Visualize 1 face per student\nplt.figure(figsize=(15, 6))\n\nfor i, sid in enumerate(sorted(df['student_id'].unique())):\n    # Get first image path for each student\n    sample_row = df[df['student_id'] == sid].iloc[0]\n    path = sample_row['path']\n    \n    img = cv2.imread(path)\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)  # Convert to Grayscale\n    faces = face_cascade.detectMultiScale(gray, 1.1, 5)\n\n    # Try detecting face\n    if len(faces) > 0:\n        (x, y, w, h) = faces[0]\n        face = gray[y:y+h, x:x+w]\n        face = cv2.resize(face, (100, 100))\n    else:\n        face = cv2.resize(gray, (100, 100))  # fallback\n\n    # Show extracted face\n    plt.subplot(2, 6, i+1)\n    plt.imshow(face,cmap='gray')\n    plt.title(f\"Student {std_id}\")\n    plt.axis('off')\n\nplt.suptitle(\"One Face per Student (Grayscale, Cropped)\", fontsize=16)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T17:57:47.670221Z","iopub.execute_input":"2025-07-22T17:57:47.670533Z","iopub.status.idle":"2025-07-22T17:57:50.764018Z","shell.execute_reply.started":"2025-07-22T17:57:47.67051Z","shell.execute_reply":"2025-07-22T17:57:50.763142Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Face Detection Coverage","metadata":{}},{"cell_type":"code","source":"detected_faces = 0\nfor p in df['path']:\n    img = cv2.imread(p)\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    faces = face_cascade.detectMultiScale(gray, 1.1, 5)\n    if len(faces) > 0:\n        detected_faces += 1\n\nprint(f\"Faces Detected: {detected_faces} / {len(df)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T17:57:50.764991Z","iopub.execute_input":"2025-07-22T17:57:50.765322Z","iopub.status.idle":"2025-07-22T18:09:50.466554Z","shell.execute_reply.started":"2025-07-22T17:57:50.765292Z","shell.execute_reply":"2025-07-22T18:09:50.465869Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# pie chart\nplt.pie([detected_faces, len(df)-detected_faces], \n        labels=[\"Detected\", \"Not Detected\"], \n        autopct='%1.1f%%', colors=['green', 'red'])\nplt.title(\"Face Detection Coverage\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T18:09:50.46722Z","iopub.execute_input":"2025-07-22T18:09:50.467488Z","iopub.status.idle":"2025-07-22T18:09:50.699412Z","shell.execute_reply.started":"2025-07-22T18:09:50.467468Z","shell.execute_reply":"2025-07-22T18:09:50.698578Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Image Quality Analysis\n# Aspect Ratio Insight","metadata":{}},{"cell_type":"code","source":"# Aspect Ratio per image\nplt.figure(figsize=(10, 4))\ndf.boxplot(column='aspect_ratio', by='student_id')\nplt.title(\"Aspect Ratio per Student\")\nplt.suptitle(\"\")\nplt.xlabel(\"Student ID\")\nplt.ylabel(\"Aspect Ratio (Width / Height)\")\nplt.grid(True)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T18:09:50.700308Z","iopub.execute_input":"2025-07-22T18:09:50.700651Z","iopub.status.idle":"2025-07-22T18:09:50.942549Z","shell.execute_reply.started":"2025-07-22T18:09:50.700631Z","shell.execute_reply":"2025-07-22T18:09:50.941643Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Bright vs Contrast Insight","metadata":{}},{"cell_type":"code","source":"# Brightness vs Contrast\nplt.figure(figsize=(10, 4))\nfor std_id in sorted(df['student_id'].unique()):\n    sub = df[df['student_id'] == std_id]\n    plt.scatter(sub['brightness'], sub['contrast'], label=std_id, alpha=0.5)\n\nplt.xlabel(\"Brightness (Mean Pixel Intensity)\")\nplt.ylabel(\"Contrast (Pixel Intensity Std Dev)\")\nplt.title(\"Brightness vs Contrast per Student\")\nplt.legend(title=\"Student ID\", bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.grid(True)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T18:09:50.945034Z","iopub.execute_input":"2025-07-22T18:09:50.945305Z","iopub.status.idle":"2025-07-22T18:09:51.421506Z","shell.execute_reply.started":"2025-07-22T18:09:50.945284Z","shell.execute_reply":"2025-07-22T18:09:51.420563Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Sharpness of the Image","metadata":{}},{"cell_type":"code","source":"# Blur Score of Images\nplt.figure(figsize=(10, 4))\ndf.boxplot(column='blur_score', by='student_id')\nplt.title(\"Image Sharpness (Blur Score)\")\nplt.suptitle(\"\")\nplt.xlabel(\"Student ID\")\nplt.ylabel(\"Blur Score (Variance of Laplacian)\")\nplt.grid(True)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T18:09:51.422373Z","iopub.execute_input":"2025-07-22T18:09:51.422591Z","iopub.status.idle":"2025-07-22T18:09:51.69664Z","shell.execute_reply.started":"2025-07-22T18:09:51.422574Z","shell.execute_reply":"2025-07-22T18:09:51.695624Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Size Distribution","metadata":{}},{"cell_type":"code","source":"# Image size distribution\nplt.figure(figsize=(10, 4))\nplt.hist(df['width'] * df['height'], bins=20, color='skyblue')\nplt.title(\"Image Size Distribution (in pixels)\")\nplt.xlabel(\"Image Size (Width Ã— Height)\")\nplt.ylabel(\"Number of Images\")\nplt.grid(True)\nplt.show()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T18:09:51.697744Z","iopub.execute_input":"2025-07-22T18:09:51.698092Z","iopub.status.idle":"2025-07-22T18:09:51.932305Z","shell.execute_reply.started":"2025-07-22T18:09:51.698067Z","shell.execute_reply":"2025-07-22T18:09:51.931221Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Feature Extraction:\n# HAAR Cascade Facial Features Detection","metadata":{}},{"cell_type":"code","source":"# Load image\nsample_path = df.iloc[1450]['path']\nimg = cv2.imread(sample_path)\ngray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n# Detect face\nfaces = face_cascade.detectMultiScale(gray, 1.3, 5)\nfor (x, y, w, h) in faces:\n    cv2.rectangle(img, (x, y), (x+w, y+h), (255, 0, 0), 2)\n    \n    roi_gray = gray[y:y+h, x:x+w]\n    roi_color = img[y:y+h, x:x+w]\n    \n    # Eyes\n    eyes = eye_cascade.detectMultiScale(roi_gray, 1.1, 10)\n    for (ex, ey, ew, eh) in eyes:\n        cv2.rectangle(roi_color, (ex, ey), (ex+ew, ey+eh), (0, 255, 255), 2)\n        \n    # Smile\n    smiles = smile_cascade.detectMultiScale(roi_gray, 1.7, 22)\n    for (sx, sy, sw, sh) in smiles:\n        cv2.rectangle(roi_color, (sx, sy), (sx+sw, sy+sh), (0, 255, 0), 2)\n    \n    # # Nose\n    # if not nose_cascade.empty():\n    #     noses = nose_cascade.detectMultiScale(roi_gray, 1.3, 5)\n    #     for (nx, ny, nw, nh) in noses:\n    #         cv2.rectangle(roi_color, (nx, ny), (nx+nw, ny+nh), (255, 0, 255), 2)\n\n# Show image using matplotlib\nplt.figure(figsize=(8, 6))\nplt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\nplt.title(\"Facial Features Detection\")\nplt.axis('off')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T18:09:51.933417Z","iopub.execute_input":"2025-07-22T18:09:51.933676Z","iopub.status.idle":"2025-07-22T18:09:52.343447Z","shell.execute_reply.started":"2025-07-22T18:09:51.93365Z","shell.execute_reply":"2025-07-22T18:09:52.342247Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Edge Detection Using HOG","metadata":{}},{"cell_type":"code","source":"# --- HOG Visualization ---\n\n# Pick one example face\nsample_path = df.iloc[1450]['path']\nimg = cv2.imread(sample_path)\ngray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\nfaces = face_cascade.detectMultiScale(gray, 1.1, 5)\n\n(x, y, w, h) = max(faces, key=lambda r: r[2]*r[3])\nface = gray[y:y+h, x:x+w]\nface = cv2.resize(face, (100, 100))\nnorm = face.astype('float32') / 255.0\n\n# Extract HOG features and image\nfeatures, hog_image = hog(norm, pixels_per_cell=(8, 8), cells_per_block=(2, 2),\n                          visualize=True, feature_vector=True)\n\n# Enhance contrast\nhog_image = exposure.rescale_intensity(hog_image, in_range=(0, 10))\n\n# Original Face\nplt.figure(figsize=(10,4))\nplt.subplot(1,2,1)\nplt.imshow(norm, cmap='gray')\nplt.title(\"Original Face\")\nplt.axis('off')\n\n# Hog\nplt.subplot(1,2,2)\nplt.imshow(hog_image, cmap='gray')\nplt.title(\"HOG Visualization\")\nplt.axis('off')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T18:09:52.344839Z","iopub.execute_input":"2025-07-22T18:09:52.345198Z","iopub.status.idle":"2025-07-22T18:09:52.722725Z","shell.execute_reply.started":"2025-07-22T18:09:52.34517Z","shell.execute_reply":"2025-07-22T18:09:52.721802Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Data Splitting - Train&Test\nX_train, X_test, y_train, y_test = [], [], [], []\nle = LabelEncoder()\n\nprint(\"Extracting HOG features...\")\nfor std_id in student_ids:\n    feats = []\n    paths = sorted(glob(os.path.join(base_path, std_id,std_id, '*.jpg')))\n    for p in paths:\n        img = cv2.imread(p)\n        if img is None:\n            continue\n        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n        faces = face_cascade.detectMultiScale(gray, 1.1, 5)\n        if len(faces) == 0:\n            continue\n        x, y, w, h = max(faces, key=lambda r: r[2] * r[3])\n        face = gray[y:y + h, x:x + w]\n        if np.var(face) < 100:\n            continue\n        face = cv2.resize(face, (100, 100)).astype('float32') / 255.0\n        feats.append(hog(face, pixels_per_cell=(8, 8), cells_per_block=(2, 2), feature_vector=True))\n    \n    if len(feats) >= 2:\n        a, b = train_test_split(feats, test_size=0.5, random_state=42)\n        X_train += a\n        X_test += b\n        y_train += [std_id] * len(a)\n        y_test += [std_id] * len(b)\n\n# Encode the labels\ny_train_enc = le.fit_transform(y_train)\ny_test_enc = le.transform(y_test)\n\nprint(\"Training samples:\", len(X_train))\nprint(\"Testing samples:\", len(X_test))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T18:26:00.690431Z","iopub.execute_input":"2025-07-22T18:26:00.691382Z","iopub.status.idle":"2025-07-22T18:38:16.040964Z","shell.execute_reply.started":"2025-07-22T18:26:00.691352Z","shell.execute_reply":"2025-07-22T18:38:16.040146Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Model Training\nprint(\"Training SVM...\")\nmodel = SVC(kernel='linear', probability=True)\nmodel.fit(X_train, y_train_enc)\nprint(\"Training complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T18:38:16.042107Z","iopub.execute_input":"2025-07-22T18:38:16.042422Z","iopub.status.idle":"2025-07-22T18:38:28.208059Z","shell.execute_reply.started":"2025-07-22T18:38:16.042393Z","shell.execute_reply":"2025-07-22T18:38:28.20713Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Evaluation ---\ny_pred = model.predict(X_test)\nacc = accuracy_score(y_test_enc, y_pred)\nprint(f\"Test Accuracy: {acc:.2%}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T18:45:22.786992Z","iopub.execute_input":"2025-07-22T18:45:22.788031Z","iopub.status.idle":"2025-07-22T18:45:27.139681Z","shell.execute_reply.started":"2025-07-22T18:45:22.788001Z","shell.execute_reply":"2025-07-22T18:45:27.138823Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Confusion Matrix\nConfusionMatrixDisplay(confusion_matrix(y_test_enc, y_pred), display_labels=le.classes_).plot(cmap='Blues', xticks_rotation=45)\nplt.title(\"Confusion Matrix\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T18:45:41.430847Z","iopub.execute_input":"2025-07-22T18:45:41.431557Z","iopub.status.idle":"2025-07-22T18:45:41.833393Z","shell.execute_reply.started":"2025-07-22T18:45:41.431527Z","shell.execute_reply":"2025-07-22T18:45:41.832296Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(classification_report(y_test_enc, y_pred, target_names=le.classes_))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T18:52:51.239595Z","iopub.execute_input":"2025-07-22T18:52:51.240414Z","iopub.status.idle":"2025-07-22T18:52:51.258708Z","shell.execute_reply.started":"2025-07-22T18:52:51.24039Z","shell.execute_reply":"2025-07-22T18:52:51.257651Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Generate the classification report dictionary\nreport_dict = classification_report(y_test_enc, y_pred, target_names=le.classes_, output_dict=True)\n\n# Convert to DataFrame\nreport_df = pd.DataFrame(report_dict).transpose()\n\n# Keep only student IDs\nreport_df = report_df[report_df.index.isin(le.classes_)]\n\n# Round for display\nreport_df[['precision', 'recall', 'f1-score']] = report_df[['precision', 'recall', 'f1-score']].round(2)\n\n# Plotting with support as annotation\nax = report_df[['precision', 'recall', 'f1-score']].plot(kind='bar', figsize=(12, 6), colormap='Set2')\nplt.title(\"Classification Metrics per Student\")\nplt.ylabel(\"Score\")\nplt.ylim(0, 1.1)\nplt.grid(axis='y', linestyle='--', alpha=0.5)\nplt.xticks(rotation=0)\n\n# Add support (sample count) as labels above each group\nfor i, (idx, row) in enumerate(report_df.iterrows()):\n    support = int(row['support'])\n    plt.text(i, 1.05, f'n={support}', ha='center', fontsize=9, color='black')\n\nplt.tight_layout()\nplt.legend(loc='upper right')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T18:52:30.370714Z","iopub.execute_input":"2025-07-22T18:52:30.371363Z","iopub.status.idle":"2025-07-22T18:52:30.731036Z","shell.execute_reply.started":"2025-07-22T18:52:30.371338Z","shell.execute_reply":"2025-07-22T18:52:30.73011Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"report_df['support'].plot(kind='bar', color='lightblue')\nplt.title(\"Number of Test Images per Student\")\nplt.ylabel(\"Support (count)\")\nplt.xticks(rotation=0)\nplt.grid(axis='y')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T18:53:38.288441Z","iopub.execute_input":"2025-07-22T18:53:38.288767Z","iopub.status.idle":"2025-07-22T18:53:38.52023Z","shell.execute_reply.started":"2025-07-22T18:53:38.288745Z","shell.execute_reply":"2025-07-22T18:53:38.51932Z"}},"outputs":[],"execution_count":null}]}